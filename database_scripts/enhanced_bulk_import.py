"""
æ”¹é€²çš„æ‰¹é‡å°å…¥ç®¡ç†å™¨
æ”¯æ´è©³ç´°æ—¥èªŒè¨˜éŒ„å’Œå¤±æ•—æ–‡ä»¶è¿½è¹¤
"""

import os
import logging
from datetime import datetime
from pathlib import Path
import glob
from concurrent.futures import ThreadPoolExecutor, as_completed
from data_importer import DataImporter
from database_config import DatabaseManager, SymbolManager

class EnhancedBulkImportManager:
    """å¢å¼·ç‰ˆæ‰¹é‡å°å…¥ç®¡ç†å™¨ - æ”¯æ´è©³ç´°æ—¥èªŒå’Œå¤±æ•—è¿½è¹¤"""
    
    def __init__(self, db_manager=None, action_name="bulk_import"):
        self.db = db_manager or DatabaseManager()
        self.importer = DataImporter(self.db)
        self.symbol_manager = SymbolManager(self.db)
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'successful_files': 0,
            'failed_files': 0,
            'total_records': 0,
            'successful_symbols': set(),
            'failed_files_list': [],
            'start_time': None,
            'end_time': None
        }
        
        # è¨­ç½®å°ˆç”¨æ—¥èªŒ
        self.setup_detailed_logging(action_name)
    
    def setup_detailed_logging(self, action_name):
        """è¨­ç½®è©³ç´°æ—¥èªŒè¨˜éŒ„"""
        # å‰µå»ºæ—¥èªŒæ–‡ä»¶åï¼š{action_name}_{ä»Šå¤©æ—¥æœŸ}.log
        today = datetime.now().strftime('%Y%m%d')
        log_filename = f"{action_name}_{today}.log"
        log_path = os.path.join(os.path.dirname(__file__), 'logs', log_filename)
        
        # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(log_path), exist_ok=True)
        
        # å‰µå»ºå°ˆç”¨æ—¥èªŒè¨˜éŒ„å™¨
        self.bulk_logger = logging.getLogger(f'bulk_import_{action_name}')
        self.bulk_logger.setLevel(logging.INFO)
        
        # é¿å…é‡è¤‡è™•ç†å™¨
        if not self.bulk_logger.handlers:
            # æ–‡ä»¶è™•ç†å™¨
            file_handler = logging.FileHandler(log_path, encoding='utf-8')
            file_formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(file_formatter)
            self.bulk_logger.addHandler(file_handler)
            
            # æ§åˆ¶å°è™•ç†å™¨
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            self.bulk_logger.addHandler(console_handler)
        
        self.log_file = log_path
        self.bulk_logger.info(f"=== æ‰¹é‡å°å…¥ä»»å‹™é–‹å§‹ ===")
        self.bulk_logger.info(f"æ—¥èªŒæ–‡ä»¶: {log_path}")
    
    def scan_directory_structure(self, base_directory):
        """æƒæä¸¦åˆ†æç›®éŒ„çµæ§‹"""
        self.bulk_logger.info(f"æƒæç›®éŒ„çµæ§‹: {base_directory}")
        
        structure_info = {
            'symbols': set(),
            'intervals': set(),
            'file_types': set(),
            'total_files': 0,
            'directories': []
        }
        
        # æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
        file_patterns = ["*.csv", "*.zip", "*.parquet", "*.gz", "*.feather", "*.h5"]
        
        for root, dirs, files in os.walk(base_directory):
            if files:  # åªè¨˜éŒ„æœ‰æ–‡ä»¶çš„ç›®éŒ„
                structure_info['directories'].append(root)
                
                # åˆ†æè·¯å¾‘çµæ§‹æå–ä¿¡æ¯
                path_parts = Path(root).parts
                
                # æå–äº¤æ˜“å°åç¨±ï¼ˆé€šå¸¸åœ¨è·¯å¾‘ä¸­ï¼‰
                for part in path_parts:
                    if any(currency in part for currency in ['USDT', 'BTC', 'ETH', 'USD']):
                        structure_info['symbols'].add(part)
                    
                    # æå–æ™‚é–“é–“éš”
                    if part in ['1m', '5m', '15m', '30m', '1h', '4h', '1d']:
                        structure_info['intervals'].add(part)
                
                # çµ±è¨ˆæ–‡ä»¶
                for pattern in file_patterns:
                    matching_files = glob.glob(os.path.join(root, pattern))
                    for file_path in matching_files:
                        structure_info['total_files'] += 1
                        file_ext = Path(file_path).suffix.lower()
                        structure_info['file_types'].add(file_ext)
        
        # è¨˜éŒ„çµ±è¨ˆä¿¡æ¯
        self.bulk_logger.info(f"ç™¼ç¾çš„äº¤æ˜“å°: {len(structure_info['symbols'])} å€‹")
        self.bulk_logger.info(f"äº¤æ˜“å°åˆ—è¡¨: {sorted(structure_info['symbols'])}")
        self.bulk_logger.info(f"æ™‚é–“é–“éš”: {sorted(structure_info['intervals'])}")
        self.bulk_logger.info(f"æ–‡ä»¶æ ¼å¼: {sorted(structure_info['file_types'])}")
        self.bulk_logger.info(f"ç¸½æ–‡ä»¶æ•¸: {structure_info['total_files']}")
        self.bulk_logger.info(f"åŒ…å«æ–‡ä»¶çš„ç›®éŒ„æ•¸: {len(structure_info['directories'])}")
        
        return structure_info
    
    def import_all_data(self, base_directory, trading_types=["um"], max_workers=4):
        """å°å…¥æ‰€æœ‰è³‡æ–™ - å¢å¼·ç‰ˆ"""
        self.stats['start_time'] = datetime.now()
        
        try:
            self.bulk_logger.info("=== é–‹å§‹æ‰¹é‡å°å…¥æ‰€æœ‰è³‡æ–™ ===")
            self.bulk_logger.info(f"åŸºç¤ç›®éŒ„: {base_directory}")
            self.bulk_logger.info(f"äº¤æ˜“é¡å‹: {trading_types}")
            self.bulk_logger.info(f"ä¸¦è¡Œç·šç¨‹æ•¸: {max_workers}")
            
            # å…ˆæƒæç›®éŒ„çµæ§‹
            structure_info = self.scan_directory_structure(base_directory)
            self.stats['total_files'] = structure_info['total_files']
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦è™•ç†çš„ç›®éŒ„
            all_directories = []
            
            for trading_type in trading_types:
                self.bulk_logger.info(f"=== è™•ç†äº¤æ˜“é¡å‹: {trading_type} ===")
                
                if trading_type == "spot":
                    data_path = os.path.join(base_directory, "data", "spot")
                else:
                    data_path = os.path.join(base_directory, "data", "futures", trading_type)
                
                if os.path.exists(data_path):
                    directories = self._collect_import_directories(data_path, trading_type)
                    all_directories.extend(directories)
                    self.bulk_logger.info(f"æ‰¾åˆ° {len(directories)} å€‹æ•¸æ“šç›®éŒ„")
                else:
                    self.bulk_logger.warning(f"è·¯å¾‘ä¸å­˜åœ¨: {data_path}")
            
            # æ‰¹é‡è™•ç†æ‰€æœ‰ç›®éŒ„
            if all_directories:
                self.bulk_logger.info(f"=== é–‹å§‹ä¸¦è¡Œè™•ç† {len(all_directories)} å€‹ç›®éŒ„ ===")
                self._process_directories_parallel(all_directories, max_workers)
            
            # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
            self._log_final_statistics()
            
        except Exception as e:
            self.bulk_logger.error(f"æ‰¹é‡å°å…¥å¤±æ•—: {e}")
            raise
        
        finally:
            self.stats['end_time'] = datetime.now()
    
    def _collect_import_directories(self, data_path, trading_type):
        """æ”¶é›†éœ€è¦å°å…¥çš„ç›®éŒ„"""
        directories = []
        
        try:
            # è™•ç† daily å’Œ monthly è³‡æ–™
            for time_period in ["daily", "monthly"]:
                period_path = os.path.join(data_path, time_period)
                if os.path.exists(period_path):
                    period_dirs = self._collect_period_directories(period_path, trading_type, time_period)
                    directories.extend(period_dirs)
        
        except Exception as e:
            self.bulk_logger.error(f"æ”¶é›† {trading_type} ç›®éŒ„å¤±æ•—: {e}")
        
        return directories
    
    def _collect_period_directories(self, period_path, trading_type, time_period):
        """æ”¶é›†ç‰¹å®šæ™‚æœŸçš„ç›®éŒ„"""
        directories = []
        
        try:
            # éæ­·æ‰€æœ‰è³‡æ–™é¡å‹ç›®éŒ„
            for data_type_dir in os.listdir(period_path):
                data_type_path = os.path.join(period_path, data_type_dir)
                
                if os.path.isdir(data_type_path):
                    # éæ­¸æ”¶é›†æ‰€æœ‰åŒ…å«æ–‡ä»¶çš„å­ç›®éŒ„
                    for root, dirs, files in os.walk(data_type_path):
                        if files:  # åªè™•ç†åŒ…å«æ–‡ä»¶çš„ç›®éŒ„
                            # æª¢æŸ¥æ˜¯å¦æœ‰æ”¯æ´çš„æ–‡ä»¶æ ¼å¼
                            supported_files = []
                            file_patterns = ["*.csv", "*.zip", "*.parquet", "*.gz", "*.feather", "*.h5"]
                            for pattern in file_patterns:
                                supported_files.extend(glob.glob(os.path.join(root, pattern)))
                            
                            if supported_files:
                                directories.append({
                                    'path': root,
                                    'trading_type': trading_type,
                                    'time_period': time_period,
                                    'data_type': data_type_dir,
                                    'file_count': len(supported_files)
                                })
        
        except Exception as e:
            self.bulk_logger.error(f"æ”¶é›† {time_period} ç›®éŒ„å¤±æ•—: {e}")
        
        return directories
    
    def _process_directories_parallel(self, directories, max_workers):
        """ä¸¦è¡Œè™•ç†ç›®éŒ„"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_dir = {
                executor.submit(self._process_single_directory, dir_info): dir_info
                for dir_info in directories
            }
            
            for future in as_completed(future_to_dir):
                dir_info = future_to_dir[future]
                try:
                    result = future.result()
                    if result['success']:
                        self.stats['successful_files'] += result['successful_files']
                        self.stats['total_records'] += result['total_records']
                        self.stats['successful_symbols'].add(result['symbol'])
                    else:
                        self.stats['failed_files'] += result['failed_files']
                        self.stats['failed_files_list'].extend(result['failed_files_list'])
                    
                except Exception as e:
                    self.bulk_logger.error(f"è™•ç†ç›®éŒ„å¤±æ•— {dir_info['path']}: {e}")
                    self.stats['failed_files'] += dir_info['file_count']
    
    def _process_single_directory(self, dir_info):
        """è™•ç†å–®å€‹ç›®éŒ„"""
        path = dir_info['path']
        trading_type = dir_info['trading_type']
        
        # å¾è·¯å¾‘ä¸­æå–äº¤æ˜“å°åç¨±
        path_parts = Path(path).parts
        symbol = None
        for part in path_parts:
            if any(currency in part for currency in ['USDT', 'BTC', 'ETH', 'USD']):
                symbol = part
                break
        
        result = {
            'success': True,
            'successful_files': 0,
            'failed_files': 0,
            'total_records': 0,
            'symbol': symbol or 'UNKNOWN',
            'failed_files_list': []
        }
        
        try:
            self.bulk_logger.info(f"è™•ç†ç›®éŒ„: {path}")
            
            # ä½¿ç”¨ DataImporter çš„ import_directory æ–¹æ³•
            original_method = self.importer.import_directory
            
            # æš«æ™‚é‡å¯«æ–¹æ³•ä¾†æ”¶é›†çµ±è¨ˆä¿¡æ¯
            def enhanced_import_directory(directory_path, file_patterns=None, max_workers=1):
                try:
                    if file_patterns is None:
                        file_patterns = ["*.csv", "*.zip", "*.parquet", "*.gz", "*.feather", "*.h5"]
                    elif isinstance(file_patterns, str):
                        file_patterns = [file_patterns]
                    
                    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
                    all_files = []
                    for pattern in file_patterns:
                        files = glob.glob(os.path.join(directory_path, "**", pattern), recursive=True)
                        all_files.extend(files)
                    
                    all_files = list(set(all_files))
                    
                    if not all_files:
                        return
                    
                    # è™•ç†æ¯å€‹æ–‡ä»¶
                    for file_path in all_files:
                        try:
                            success = self.importer.import_single_file(file_path, trading_type)
                            if success:
                                result['successful_files'] += 1
                                # é€™è£¡å¯ä»¥æ·»åŠ è¨˜éŒ„æ•¸çµ±è¨ˆ
                            else:
                                result['failed_files'] += 1
                                result['failed_files_list'].append(file_path)
                                
                        except Exception as file_error:
                            self.bulk_logger.error(f"æ–‡ä»¶è™•ç†å¤±æ•— {file_path}: {file_error}")
                            result['failed_files'] += 1
                            result['failed_files_list'].append(file_path)
                            result['success'] = False
                
                except Exception as e:
                    self.bulk_logger.error(f"ç›®éŒ„è™•ç†å¤±æ•— {directory_path}: {e}")
                    result['success'] = False
            
            # åŸ·è¡Œå¢å¼·çš„å°å…¥
            enhanced_import_directory(path)
            
        except Exception as e:
            self.bulk_logger.error(f"è™•ç†ç›®éŒ„å¤±æ•— {path}: {e}")
            result['success'] = False
        
        return result
    
    def _log_final_statistics(self):
        """è¨˜éŒ„æœ€çµ‚çµ±è¨ˆä¿¡æ¯"""
        duration = self.stats['end_time'] - self.stats['start_time'] if self.stats['end_time'] else datetime.now() - self.stats['start_time']
        
        self.bulk_logger.info("=== æ‰¹é‡å°å…¥å®Œæˆ - æœ€çµ‚çµ±è¨ˆ ===")
        self.bulk_logger.info(f"ç¸½è™•ç†æ™‚é–“: {duration}")
        self.bulk_logger.info(f"ç¸½æ–‡ä»¶æ•¸: {self.stats['total_files']}")
        self.bulk_logger.info(f"æˆåŠŸæ–‡ä»¶æ•¸: {self.stats['successful_files']}")
        self.bulk_logger.info(f"å¤±æ•—æ–‡ä»¶æ•¸: {self.stats['failed_files']}")
        self.bulk_logger.info(f"æˆåŠŸç‡: {(self.stats['successful_files'] / max(self.stats['total_files'], 1) * 100):.2f}%")
        self.bulk_logger.info(f"ç¸½è¨˜éŒ„æ•¸: {self.stats['total_records']}")
        self.bulk_logger.info(f"æˆåŠŸè™•ç†çš„äº¤æ˜“å°: {len(self.stats['successful_symbols'])} å€‹")
        self.bulk_logger.info(f"äº¤æ˜“å°åˆ—è¡¨: {sorted(self.stats['successful_symbols'])}")
        
        # è¨˜éŒ„å¤±æ•—æ–‡ä»¶
        if self.stats['failed_files_list']:
            self.bulk_logger.error("=== å¤±æ•—æ–‡ä»¶åˆ—è¡¨ ===")
            for failed_file in self.stats['failed_files_list']:
                self.bulk_logger.error(f"å¤±æ•—æ–‡ä»¶: {failed_file}")
        
        # æ§åˆ¶å°é¡¯ç¤ºæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š æ‰¹é‡å°å…¥å®Œæˆæ‘˜è¦")
        print("="*60)
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {duration}")
        print(f"ğŸ“ ç¸½æ–‡ä»¶æ•¸: {self.stats['total_files']}")
        print(f"âœ… æˆåŠŸæ–‡ä»¶æ•¸: {self.stats['successful_files']}")
        print(f"âŒ å¤±æ•—æ–‡ä»¶æ•¸: {self.stats['failed_files']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(self.stats['successful_files'] / max(self.stats['total_files'], 1) * 100):.2f}%")
        print(f"ğŸ“„ æ—¥èªŒæ–‡ä»¶: {self.log_file}")
        
        if self.stats['failed_files_list']:
            print(f"\nâŒ å¤±æ•—æ–‡ä»¶åˆ—è¡¨ ({len(self.stats['failed_files_list'])} å€‹):")
            for i, failed_file in enumerate(self.stats['failed_files_list'][:10]):  # åªé¡¯ç¤ºå‰10å€‹
                print(f"   {i+1}. {failed_file}")
            if len(self.stats['failed_files_list']) > 10:
                print(f"   ... åŠå…¶ä»– {len(self.stats['failed_files_list']) - 10} å€‹æ–‡ä»¶")
            print(f"å®Œæ•´å¤±æ•—åˆ—è¡¨è«‹æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶: {self.log_file}")
        
        print("="*60)
